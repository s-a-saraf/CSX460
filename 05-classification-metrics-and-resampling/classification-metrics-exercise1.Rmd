<<<<<<< HEAD
---
title: "Classification Metrics"
author: "Christopher Brown"
date: 
output: html_document
---

The German Credit Data (data/german_credit); 

1. Read in the German Credit Data
2. Partition the model into Test and Training Sets using only `base::sample` 
3. Train a model for `Creditability` 
4. Knit the document and submit both the this file Rmd and html files.

Show All Work! 

```{r}
setwd("~/csx460/CSX460/05-classification-metrics-and-resampling")
library(dplyr)
library(readr)
library(ggplot2)
library(magrittr)
library(lubridate)
df <- read_csv("../data/german_credit.csv")
str(df)

# Check for unique values < Columns which single value and hence can be ignored >
df %>% sapply(. %>% unique %>% length)

# Find the NA count for each feature
df %>% sapply(. %>% is.na %>% sum)
library(Amelia)
missmap(df)

#split the data into training and test sets.
vari <- sample(1:nrow(df), 0.75 * nrow(df), replace = FALSE)
df_train <- df[vari, ]
df_test <- df[-vari, ]

logisticModel <- glm(Creditability ~ ., data = df_train, family = "binomial")

#Analyze the model
summary(logisticModel)
```


Using the `predict` function and `test` data, write functions to calculate and 
calculate: 

* Misclassification Rate
* Prevalence 
* Accuracy
* Accuracy
* Error Rate / Misclassification Rate
* True Positive Rate  
* False Positive Rate
* True Negative Rate  
* False Negative Rate 
* Sensitivity 
* Specificity 
* Recall 
* Precision

```{r, echo=FALSE}
predData <- predict(logisticModel, df_test, type = "response")
hist(predData)
predictTable <- data.frame(predictType = predData, actualType = df_test$Creditability)
summary(predictTable)
# Apply ROC to get the threshold
library(pROC)
analysis <- roc(response=predictTable$actualType, predictor=predictTable$predictType)
e <- cbind(analysis$thresholds,analysis$sensitivities+analysis$specificities)
opt_t <- subset(e,e[,2]==max(e[,2]))[,1]

#Plot ROC Curve
plot(1-analysis$specificities,analysis$sensitivities,type="l",
ylab="Sensitiviy",xlab="1-Specificity",col="black",lwd=2,
main = "ROC Curve for Simulated Data")
abline(a=0,b=1)
abline(v = opt_t) #add optimal t to ROC curve
opt_t #print t

# O/P from above -
#threshold <- 0.7616483
threshold <- 0.025
#Generate the confusion matrix.
# < threshold = 0
# >= threshold = 1

predictTable$predictType[which(predictTable$predictType < threshold)] <- 0
predictTable$predictType[which(predictTable$predictType >= threshold)] <- 1

# Confusion Matrix generation
library(caret)
confusionMat <- confusionMatrix(predictTable$predictType, predictTable$actualType, positive = "1")
confusionMat

# TODO : How to extract the values from the confusion matrix ?
TN <- 0
FN <- 0
TP <- 0
FP <- 0

# Workaround - Loop through all and calculate - TN, TP, FN, FP
for (i in 1:nrow(predictTable)) {
    if (predictTable[i, 1] %in% c(1)) {
       if (predictTable[i, 2] %in% c(1)) {
         TP <- TP + 1
       } else {
         FP <- FP + 1
       }
    } else {
      if (predictTable[i, 2]  %in% c(1)) {
        FN <- FN + 1
      } else {
        TN <- TN + 1
      }
    }
}

  TN
  TP
  FN
  FP
# Misclassification Rate
# Prevalence 
# Accuracy
accuracy <- (TP + TN)/(TP + TN + FP + FN)
accuracy
# Error Rate / Misclassification Rate
err_rate <- (FP + FN)/(TP + TN + FP + FN)
err_rate <- 1 - accuracy
err_rate
# True Positive Rate  
TPR <- TP/(TP+FN)
TPR
# Sensitivity 
Sensitivity <- TPR
Sensitivity
# Recall 
recall <- TPR
recall
# False Positive Rate
FPR = FP/(FP + TN)
FPR
# True Negative Rate
TNR <- TN/(FP+TN)
TNR
# Specificity 
Specificity <- TNR
Specificity
# False Negative Rate 
FNR <- FN/(FN + TP)
FNR
# Precision
Precision <- TP/(TP + FP)
Precision
```
=======
---
title: "Classification Metrics"
author: "Christopher Brown"
date: 
output: html_document
---

Using the first **logistic regression** model that you created in previous exercise; 

1. Partition the model into Test and Training Sets using only `base::sample` 

```{r}

```

2. Re-train the model on the training data 

```{r}

```

3. Using the `predict` function and `test` data, write functions to calculate and 
calculate: 

* Misclassification Rate
* Prevalence 
* Accuracy
* Accuracy
* Error Rate / Misclassification Rate
* True Positive Rate  
* False Positive Rate
* True Negative Rate  
* False Negative Rate 
* Sensitivity 
* Specificity 
* Recall 
* Precision

```{r, echo=FALSE}


```
>>>>>>> c0e49b99fba8263cda933bb4efffc7dd6b4352e0
